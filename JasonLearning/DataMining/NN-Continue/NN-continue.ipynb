{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4999, 400), (4999, 1))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, scale\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_data = pd.read_csv('data/X_data.csv').to_numpy()\n",
    "y_label = pd.read_csv('data/y_label.csv').to_numpy()\n",
    "\n",
    "x_data.shape, y_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义sigmoid函数\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, x, y, classes, hidden_features):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.classifications = classes\n",
    "        self.samples, self.features = x.shape\n",
    "        self.hidden_features = hidden_features\n",
    "\n",
    "        self.weights1 = np.random.rand(self.features, self.hidden_features)\n",
    "        self.weights2 = np.random.rand(self.hidden_features, self.classifications)\n",
    "        self.bias1 = 1\n",
    "        self.bias2 = 1\n",
    "\n",
    "        self.net_hidden = None\n",
    "        self.sigmoid_hidden = None\n",
    "        self.net_out = None\n",
    "        self.sigmoid_out = None\n",
    "\n",
    "    def _forward_propagation(self, x_test=None):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 输入层---->隐含层\n",
    "        self.net_hidden = np.dot(self.x if x_test is None else x_test, self.weights1) + self.bias1\n",
    "        self.sigmoid_hidden = sigmoid(self.net_hidden)\n",
    "        # 隐含层---->输出层\n",
    "        self.net_out = np.dot(self.sigmoid_hidden, self.weights2) + self.bias2\n",
    "        self.sigmoid_out = sigmoid(self.net_out)\n",
    "\n",
    "    def _back_propagation(self, eta):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        delta_out = -(self.y - self.sigmoid_out) * (self.sigmoid_out * (1 - self.sigmoid_out))\n",
    "        theta_weight2 = np.dot(self.sigmoid_hidden.T, delta_out)\n",
    "\n",
    "        delta_hidden = np.sum(np.concatenate([(delta_out * self.weights2[i]).reshape(self.samples, 1, self.classifications) for i in range(self.hidden_features)], 1), axis=2)* (self.sigmoid_hidden * (1 - self.sigmoid_hidden))\n",
    "        theta_weight1 = np.dot(self.x.T, delta_hidden)\n",
    "\n",
    "        self.weights1 -= eta * theta_weight1  # 隐含层---->输出层的偏置项更新\n",
    "        self.bias1 -= eta * np.sum(delta_hidden)  # 隐含层---->输出层的权值更新\n",
    "        self.weights2 -= eta * theta_weight2  # 输入层---->隐含层的偏置项更新\n",
    "        self.bias2 -= eta * np.sum(delta_out)  # 输入层---->隐含层的权值更新\n",
    "\n",
    "    def fit(self, epochs=10000, eta=1e-2):\n",
    "        print(\"--------------训练开始--------------\")\n",
    "        max_score = 0\n",
    "        for i in np.arange(1, epochs + 1):\n",
    "            self.x, self.y = shuffle(self.x, self.y)\n",
    "            self._forward_propagation()\n",
    "            self._back_propagation(eta)\n",
    "\n",
    "            s = self.score()\n",
    "            if s > max_score:\n",
    "                max_score = s\n",
    "            #每100轮打印score\n",
    "            if i % 100 == 0:\n",
    "                print(i, \"Loss:\",self._calculate_square_error(),\"Score:\", s)\n",
    "        print(\"最好的准确率:\", max_score)\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        self._forward_propagation(x_test)\n",
    "        pred_zeros = np.zeros((self.samples, self.classifications))\n",
    "        for i in range(len(self.sigmoid_out)):\n",
    "            pred_zeros[i, np.argmax(self.sigmoid_out[i])] = 1\n",
    "        return pred_zeros\n",
    "\n",
    "    def score(self, x_test=None, y_test=None):\n",
    "        if x_test is None and y_test is None:\n",
    "            x_test, y_test = self.x, self.y\n",
    "        pred = [np.argmax(i) for i in self.predict(x_test)]\n",
    "        label = [np.argmax(i) for i in y_test]\n",
    "        s = 0.0\n",
    "        for i in range(len(pred)):\n",
    "            if label[i] == pred[i]:\n",
    "                s += 1\n",
    "        return s / x_test.shape[0]\n",
    "\n",
    "    def _calculate_square_error(self, x_test=None, y_test=None):\n",
    "        if x_test is None and y_test is None:\n",
    "            x_test, y_test = self.x, self.y\n",
    "        return 0.5 * np.sum((y_test - self.sigmoid_out) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征归一化\n",
    "# 标签one-hot编码\n",
    "np.random.seed(64)\n",
    "x_data = scale(x_data)\n",
    "y_label_oh = OneHotEncoder(sparse=False).fit_transform(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------训练开始--------------\n",
      "100 Loss: 2362.659242653648 Score: 0.1842368473694739\n",
      "200 Loss: 2334.450291120498 Score: 0.21824364872974594\n",
      "300 Loss: 2217.810160004977 Score: 0.27385477095419086\n",
      "400 Loss: 1849.2197589901998 Score: 0.4138827765553111\n",
      "500 Loss: 1617.9206785172291 Score: 0.511502300460092\n",
      "600 Loss: 1463.548896389634 Score: 0.5945189037807561\n",
      "700 Loss: 1337.1152972777895 Score: 0.6335267053410683\n",
      "800 Loss: 1241.9479578510122 Score: 0.6649329865973195\n",
      "900 Loss: 1152.0497805430125 Score: 0.6875375075015003\n",
      "1000 Loss: 1066.626701566244 Score: 0.7069413882776555\n",
      "1100 Loss: 985.239575459059 Score: 0.8013602720544108\n",
      "1200 Loss: 857.094733507512 Score: 0.8271654330866173\n",
      "1300 Loss: 711.5098095751126 Score: 0.8401680336067213\n",
      "1400 Loss: 631.9677858472234 Score: 0.8577715543108622\n",
      "1500 Loss: 568.3634909296445 Score: 0.8711742348469694\n",
      "1600 Loss: 516.6404258886522 Score: 0.8815763152630526\n",
      "1700 Loss: 475.4222755673038 Score: 0.8919783956791358\n",
      "1800 Loss: 442.57443661051934 Score: 0.8995799159831966\n",
      "1900 Loss: 414.3409304863809 Score: 0.9049809961992399\n",
      "2000 Loss: 391.97892886556895 Score: 0.910382076415283\n",
      "2100 Loss: 371.47010539550314 Score: 0.9147829565913183\n",
      "2200 Loss: 352.3040699847373 Score: 0.918383676735347\n",
      "2300 Loss: 336.1457996781648 Score: 0.9219843968793758\n",
      "2400 Loss: 322.5619118711121 Score: 0.9251850370074015\n",
      "2500 Loss: 309.4176510954312 Score: 0.9267853570714143\n",
      "2600 Loss: 297.18089840838053 Score: 0.9299859971994399\n",
      "2700 Loss: 286.0251093005919 Score: 0.9313862772554511\n",
      "2800 Loss: 274.812944972642 Score: 0.9345869173834767\n",
      "2900 Loss: 264.8178582359296 Score: 0.9379875975195039\n",
      "3000 Loss: 255.34812855858627 Score: 0.9409881976395279\n",
      "3100 Loss: 247.34606572847588 Score: 0.9423884776955391\n",
      "3200 Loss: 240.28569820270337 Score: 0.9447889577915584\n",
      "3300 Loss: 233.98781201005775 Score: 0.9469893978795759\n",
      "3400 Loss: 227.0392790664168 Score: 0.9473894778955791\n",
      "3500 Loss: 221.73421337322347 Score: 0.9481896379275855\n",
      "3600 Loss: 217.09144804825465 Score: 0.9489897979595919\n",
      "3700 Loss: 212.99760179555966 Score: 0.9495899179835967\n",
      "3800 Loss: 208.71943808457513 Score: 0.9505901180236047\n",
      "3900 Loss: 204.02348630756148 Score: 0.9515903180636127\n",
      "4000 Loss: 199.34215117734075 Score: 0.9529905981196239\n",
      "4100 Loss: 195.2577341248971 Score: 0.9533906781356272\n",
      "4200 Loss: 190.67356060516963 Score: 0.9541908381676335\n",
      "4300 Loss: 186.53855322176977 Score: 0.9547909581916383\n",
      "4400 Loss: 182.27614400655582 Score: 0.9561912382476495\n",
      "4500 Loss: 178.63735266190565 Score: 0.9563912782556512\n",
      "4600 Loss: 175.43072224244932 Score: 0.957991598319664\n",
      "4700 Loss: 172.4286910075533 Score: 0.9583916783356672\n",
      "4800 Loss: 169.84698188114297 Score: 0.9587917583516703\n",
      "4900 Loss: 167.27816919752908 Score: 0.9593918783756752\n",
      "5000 Loss: 164.80188803249226 Score: 0.9595919183836767\n",
      "5100 Loss: 162.25313187460932 Score: 0.95999199839968\n",
      "5200 Loss: 160.2163420791152 Score: 0.9601920384076815\n",
      "5300 Loss: 158.33111782040348 Score: 0.9603920784156831\n",
      "5400 Loss: 156.51757309711473 Score: 0.9607921584316863\n",
      "5500 Loss: 154.71753580311434 Score: 0.9613922784556911\n",
      "5600 Loss: 152.95432059918105 Score: 0.9613922784556911\n",
      "5700 Loss: 151.0658795272506 Score: 0.9615923184636928\n",
      "5800 Loss: 149.41850445827856 Score: 0.9623924784956991\n",
      "5900 Loss: 147.62209195045813 Score: 0.9625925185037008\n",
      "6000 Loss: 146.02194521387636 Score: 0.9627925585117023\n",
      "6100 Loss: 144.62500222078899 Score: 0.9627925585117023\n",
      "6200 Loss: 143.3171999605872 Score: 0.9627925585117023\n",
      "6300 Loss: 141.8154753447889 Score: 0.962992598519704\n",
      "6400 Loss: 140.33328480151246 Score: 0.9631926385277055\n",
      "6500 Loss: 138.2722229100356 Score: 0.9637927585517103\n",
      "6600 Loss: 136.52305216367157 Score: 0.9641928385677135\n",
      "6700 Loss: 135.3136490598614 Score: 0.9641928385677135\n",
      "6800 Loss: 133.6501475152849 Score: 0.9647929585917183\n",
      "6900 Loss: 131.57458042122914 Score: 0.9651930386077215\n",
      "7000 Loss: 130.4988567734937 Score: 0.9651930386077215\n",
      "7100 Loss: 129.41945155682086 Score: 0.9653930786157231\n",
      "7200 Loss: 127.79220257137328 Score: 0.9657931586317263\n",
      "7300 Loss: 126.77357335243669 Score: 0.9661932386477295\n",
      "7400 Loss: 125.82802677179849 Score: 0.9661932386477295\n",
      "7500 Loss: 124.31568049658755 Score: 0.966993398679736\n",
      "7600 Loss: 123.20461267151134 Score: 0.9673934786957391\n",
      "7700 Loss: 122.04773655823607 Score: 0.9677935587117423\n",
      "7800 Loss: 120.58741054302244 Score: 0.9683936787357471\n",
      "7900 Loss: 118.45471728364976 Score: 0.968993798759752\n",
      "8000 Loss: 116.86842456408658 Score: 0.9693938787757551\n",
      "8100 Loss: 115.68665108942743 Score: 0.9697939587917583\n",
      "8200 Loss: 114.57921050462132 Score: 0.96999399879976\n",
      "8300 Loss: 113.63914837181999 Score: 0.9703940788157631\n",
      "8400 Loss: 112.66765313057304 Score: 0.9701940388077616\n",
      "8500 Loss: 111.7484775256929 Score: 0.9701940388077616\n",
      "8600 Loss: 110.62777186177865 Score: 0.9705941188237648\n",
      "8700 Loss: 109.79070693558985 Score: 0.9707941588317663\n",
      "8800 Loss: 109.09453302499195 Score: 0.9707941588317663\n",
      "8900 Loss: 108.42755354485016 Score: 0.9715943188637728\n",
      "9000 Loss: 107.79650084155016 Score: 0.9717943588717743\n",
      "9100 Loss: 107.21139001201516 Score: 0.9717943588717743\n",
      "9200 Loss: 106.04067922188881 Score: 0.9723944788957791\n",
      "9300 Loss: 105.4455843727465 Score: 0.9723944788957791\n",
      "9400 Loss: 104.58445412326606 Score: 0.9725945189037808\n",
      "9500 Loss: 103.67996791651 Score: 0.9729945989197839\n",
      "9600 Loss: 102.85443904598009 Score: 0.9731946389277856\n",
      "9700 Loss: 101.96902583620945 Score: 0.9733946789357871\n",
      "9800 Loss: 101.24799386987863 Score: 0.9735947189437888\n",
      "9900 Loss: 100.65239913291671 Score: 0.9735947189437888\n",
      "10000 Loss: 100.09985286308046 Score: 0.9735947189437888\n",
      "最好的准确率: 0.9735947189437888\n",
      "模型预测准确率: 0.9735947189437888\n"
     ]
    }
   ],
   "source": [
    "# 十分类 25个隐藏层节点个数\n",
    "nn = NN(x_data, y_label_oh, 10, 25)\n",
    "nn.fit(eta=0.001)\n",
    "print(\"模型预测准确率:\", nn.score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
